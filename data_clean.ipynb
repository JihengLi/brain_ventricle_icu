{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, os, time, re, csv, sys, io\n",
    "\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "from visualization import *\n",
    "from nibabel.orientations import (\n",
    "    io_orientation,\n",
    "    axcodes2ornt,\n",
    "    ornt_transform,\n",
    "    apply_orientation,\n",
    ")\n",
    "\n",
    "root_dir = \"/valiant02/masi/zuol1/data/cibs_brain2/\"\n",
    "dest_root_dir = \"/nfs2/harmonization/BIDS/CIBS_BRAIN2_Harmonized\"\n",
    "result_dir = \"dataset_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump(path_list, outfile):\n",
    "    with open(outfile, \"w\") as f:\n",
    "        for p in sorted(path_list):\n",
    "            f.write(str(p) + \"\\n\")\n",
    "    print(f\"Cached {len(path_list)} paths to '{outfile}'\")\n",
    "\n",
    "\n",
    "def load_paths_from_file(file_path: str) -> List[str]:\n",
    "    \"\"\"Load paths from a file.\"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return [line.strip() for line in f if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached 3730 paths to 'dataset_cache/nifti_all.txt'\n"
     ]
    }
   ],
   "source": [
    "lines = []\n",
    "for dirpath, dirnames, filenames in os.walk(Path(root_dir)):\n",
    "    if dirnames:\n",
    "        continue\n",
    "    file_type = Path(dirpath).name\n",
    "    session = Path(dirpath).parent.name\n",
    "    if session not in {\"00\", \"12\"}:\n",
    "        continue\n",
    "    for fname in filenames:\n",
    "        if fname.endswith(\".nii\") or fname.endswith(\".nii.gz\"):\n",
    "            lines.append(str(Path(dirpath) / fname))\n",
    "dump(lines, f\"{result_dir}/nifti_all.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 722 paths from nifti_nii.txt\n"
     ]
    }
   ],
   "source": [
    "lines = load_paths_from_file(f\"{result_dir}/nifti_nii.txt\")\n",
    "print(f\"Loaded {len(lines)} paths from nifti_nii.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "depths = [len(Path(p).parts) for p in lines]\n",
    "\n",
    "# Check whether the depths of all addresses are same\n",
    "print(len(set(depths)) == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached 76 paths to 'dataset_cache/subject_only_00.txt'\n",
      "Cached 37 paths to 'dataset_cache/subject_00_and_12.txt'\n",
      "Cached 22 paths to 'dataset_cache/subject_only_12.txt'\n",
      "Only baseline data (00 only): 76\n",
      "Valid data (00 + 12): 37\n",
      "Only 12-month data: 22\n"
     ]
    }
   ],
   "source": [
    "subject_sessions = defaultdict(set)\n",
    "\n",
    "for fp in lines:\n",
    "    p = Path(fp)\n",
    "    if len(p.parents) < 3:\n",
    "        continue\n",
    "    session = p.parents[1].name\n",
    "    file_type = p.parents[0].name\n",
    "    if session not in [\"00\", \"12\"] or file_type != \"nii\":\n",
    "        continue\n",
    "    subject_dir = p.parents[2]\n",
    "    subject_sessions[subject_dir].add(session)\n",
    "\n",
    "only_00 = [p for p, s in subject_sessions.items() if s == {\"00\"}]\n",
    "both_00_12 = [p for p, s in subject_sessions.items() if s == {\"00\", \"12\"}]\n",
    "only_12 = [p for p, s in subject_sessions.items() if s == {\"12\"}]\n",
    "\n",
    "dump(only_00, f\"{result_dir}/subject_only_00.txt\")\n",
    "dump(both_00_12, f\"{result_dir}/subject_00_and_12.txt\")\n",
    "dump(only_12, f\"{result_dir}/subject_only_12.txt\")\n",
    "\n",
    "print(f\"Only baseline data (00 only): {len(only_00)}\")\n",
    "print(f\"Valid data (00 + 12): {len(both_00_12)}\")\n",
    "print(f\"Only 12-month data: {len(only_12)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached 168 paths to 'dataset_cache/harmonized_all.txt'\n",
      "\n",
      " Subjects with missing data:\n",
      "  - BRA105: no BRAIN_T1-3D harmonized file in 00/proc\n",
      "  - BRA107: no BRAIN_T1-3D harmonized file in 12/proc\n",
      "  - BRA132: no BRAIN_T1-3D harmonized file in 12/proc\n",
      "  - VUM241: no BRAIN_T1-3D harmonized file in 12/proc\n"
     ]
    }
   ],
   "source": [
    "file_pat = re.compile(r\"BRAIN_T1-3D.*_n4_reg_harmonized_harmonized_fusion\\.nii\\.gz$\")\n",
    "\n",
    "good_paths: list[Path] = []\n",
    "bad_subjects: list[str] = []\n",
    "subject_roots = only_00 + both_00_12 + only_12\n",
    "\n",
    "for subj_root in subject_roots:\n",
    "    subj_root = Path(subj_root)\n",
    "    subj_id = subj_root.name\n",
    "\n",
    "    for ses in (\"00\", \"12\"):\n",
    "        ses_dir = subj_root / ses\n",
    "        if not ses_dir.is_dir():\n",
    "            continue\n",
    "        proc_dir = ses_dir / \"proc\"\n",
    "        if not proc_dir.is_dir():\n",
    "            bad_subjects.append(f\"{subj_id}: missing directory {ses}/proc\")\n",
    "            break\n",
    "        matches = [\n",
    "            p for p in proc_dir.iterdir() if p.is_file() and file_pat.search(p.name)\n",
    "        ]\n",
    "        if not matches:\n",
    "            bad_subjects.append(\n",
    "                f\"{subj_id}: no BRAIN_T1-3D harmonized file in {ses}/proc\"\n",
    "            )\n",
    "            break\n",
    "        good_paths.extend(matches)\n",
    "\n",
    "dump(good_paths, f\"{result_dir}/harmonized_all.txt\")\n",
    "\n",
    "if bad_subjects:\n",
    "    print(\"\\n Subjects with missing data:\")\n",
    "    for msg in bad_subjects:\n",
    "        print(\"  -\", msg)\n",
    "else:\n",
    "    print(\"\\n All subjects passed the file-check.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 168 paths from harmonized_all.txt\n"
     ]
    }
   ],
   "source": [
    "good_paths = load_paths_from_file(f\"{result_dir}/harmonized_all.txt\")\n",
    "print(f\"Loaded {len(good_paths)} paths from harmonized_all.txt\")\n",
    "\n",
    "\n",
    "def extract_modality(fname: str) -> str:\n",
    "    stem = fname\n",
    "    for ext in (\".nii.gz\", \".nii\"):\n",
    "        if stem.endswith(ext):\n",
    "            stem = stem[: -len(ext)]\n",
    "            break\n",
    "    try:\n",
    "        seq_part = stem.split(\"_BRAIN_\")[1]\n",
    "    except IndexError:\n",
    "        return \"\"\n",
    "    tokens = seq_part.split(\"-\")\n",
    "    return \"-\".join(tokens[:2]) if len(tokens) >= 2 else \"\"\n",
    "\n",
    "\n",
    "def parse_path(file_path: str) -> dict | None:\n",
    "    p = Path(file_path)\n",
    "    parts = p.parts\n",
    "    if len(parts) != 10:\n",
    "        raise RuntimeError(f\"File address not valid.\\nAddress: {file_path}\")\n",
    "    subject = parts[6]\n",
    "    session = parts[7]\n",
    "    scan_type = extract_modality(parts[9])\n",
    "    if scan_type not in {\"T1-3D\"}:\n",
    "        return None\n",
    "    return {\n",
    "        \"filepath\": p.resolve(),\n",
    "        \"scan_type\": scan_type,\n",
    "        \"subject_id\": f\"sub-{subject}\",\n",
    "        \"session_id\": f\"ses-{session}\",\n",
    "    }\n",
    "\n",
    "\n",
    "rows = [row for fp in good_paths if (row := parse_path(fp))]\n",
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"run\"] = (\n",
    "    df.groupby([\"scan_type\", \"subject_id\", \"session_id\"])\n",
    "    .cumcount()\n",
    "    .add(1)\n",
    "    .astype(\"string\")\n",
    ")\n",
    "\n",
    "mask = (\n",
    "    df.groupby([\"scan_type\", \"subject_id\", \"session_id\"])[\"run\"].transform(\"size\").eq(1)\n",
    ")\n",
    "df.loc[mask, \"run\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_links(row):\n",
    "    prefix = f\"{row.subject_id}_{row.session_id}\"\n",
    "    if row.run:\n",
    "        prefix += f\"_run-{row.run}\"\n",
    "    if row.scan_type == \"T1-3D\":\n",
    "        fname = f\"{prefix}_T1w\"\n",
    "        subdir = \"anat\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scan type: {row.scan_type}\")\n",
    "    base = f\"{dest_root_dir}/{row.subject_id}/{row.session_id}/{subdir}/{fname}\"\n",
    "    row[\"nii_link\"] = base + \".nii.gz\"\n",
    "    return row\n",
    "\n",
    "\n",
    "df = df.apply(make_links, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values([\"scan_type\", \"subject_id\", \"session_id\", \"run\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing here.\n"
     ]
    }
   ],
   "source": [
    "mask = (\n",
    "    (df[\"scan_type\"] == \"T1-3D\")\n",
    "    & df[\"run\"].notna()\n",
    "    & (df[\"run\"] != \"\")\n",
    "    # & (df[\"run\"] == \"\")\n",
    ")\n",
    "\n",
    "if mask.any():\n",
    "    print(df.loc[mask])\n",
    "else:\n",
    "    print(\"Nothing here.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"{result_dir}/data.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframe(path: str) -> pd.DataFrame:\n",
    "    if path.endswith(\".pkl\"):\n",
    "        return pd.read_pickle(path)\n",
    "    return pd.read_csv(path, sep=None, engine=\"python\")\n",
    "\n",
    "\n",
    "def _collect_link_pairs(df: pd.DataFrame) -> List[Tuple[str, str]]:\n",
    "    colmap = {\n",
    "        \"filepath\": \"nii_link\",\n",
    "    }\n",
    "\n",
    "    present_pairs = [\n",
    "        (src, dst)\n",
    "        for src, dst in colmap.items()\n",
    "        if src in df.columns and dst in df.columns\n",
    "    ]\n",
    "    if not present_pairs:\n",
    "        raise ValueError(\"No recognised source/target column pairs found.\")\n",
    "\n",
    "    commands = []\n",
    "    for _, row in df.iterrows():\n",
    "        for src, dst in present_pairs:\n",
    "            s, d = row[src], row[dst]\n",
    "            if pd.notna(s) and pd.notna(d) and str(s).strip() and str(d).strip():\n",
    "                commands.append(f'mkdir -p \"{Path(d).parent}\"')\n",
    "                commands.append(f'ln -s \"{s}\" \"{d}\"')\n",
    "    return commands\n",
    "\n",
    "\n",
    "def write_link_commands(\n",
    "    df: pd.DataFrame,\n",
    "    output_txt: str | Path,\n",
    "    *,\n",
    "    overwrite: bool = True,\n",
    ") -> None:\n",
    "    output_txt = Path(output_txt)\n",
    "    mode = \"w\" if overwrite else \"a\"\n",
    "\n",
    "    commands = _collect_link_pairs(df)\n",
    "\n",
    "    with output_txt.open(mode, encoding=\"utf-8\") as f:\n",
    "        for cmd in commands:\n",
    "            f.write(cmd + \"\\n\")\n",
    "\n",
    "    print(f\"{len(commands)} link commands written to {output_txt.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataframe(f\"{result_dir}/data.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336 link commands written to /nfs/ForHenry/brain_ventricle/dataset_cache/data_link_command.txt\n"
     ]
    }
   ],
   "source": [
    "write_link_commands(df, f\"{result_dir}/data_link_command.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pages written to dataset_cache/Harmonization_QC.pdf\n"
     ]
    }
   ],
   "source": [
    "def fig_to_ndarray(fig):\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format=\"png\", dpi=fig.dpi, bbox_inches=\"tight\")\n",
    "    buf.seek(0)\n",
    "    img = plt.imread(buf)\n",
    "    buf.close()\n",
    "    return img\n",
    "\n",
    "\n",
    "dpi = 100\n",
    "pdf_path = Path(result_dir) / \"Harmonization_QC.pdf\"\n",
    "with PdfPages(pdf_path) as pdf:\n",
    "    for file_path in df[\"filepath\"]:\n",
    "        p = Path(file_path)\n",
    "        subj = p.parts[6]\n",
    "        sess = p.parts[7]\n",
    "\n",
    "        ori_path = (\n",
    "            Path(*p.parts[:8]) / \"nii\" / (p.parts[9].split(\"_n4_reg\")[0] + \".nii.gz\")\n",
    "        )\n",
    "\n",
    "        fig_ori = visualize_t1w(ori_path, show_img=False)\n",
    "        fig_ham = visualize_t1w(p, show_img=False)\n",
    "\n",
    "        img_ori = fig_to_ndarray(fig_ori)\n",
    "        img_ham = fig_to_ndarray(fig_ham)\n",
    "\n",
    "        plt.close(fig_ori)\n",
    "        plt.close(fig_ham)\n",
    "\n",
    "        h1, w1 = img_ori.shape[:2]\n",
    "        h2, w2 = img_ham.shape[:2]\n",
    "        width = max(w1, w2)\n",
    "        height = h1 + h2\n",
    "\n",
    "        fig_h, fig_w = height / dpi, width / dpi\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(fig_w, fig_h), dpi=dpi)\n",
    "\n",
    "        axes[0].imshow(img_ori)\n",
    "        axes[0].set_title(\"Original T1-3D\", fontsize=9)\n",
    "        axes[1].imshow(img_ham)\n",
    "        axes[1].set_title(\"Harmonized T1-3D\", fontsize=9)\n",
    "        for ax in axes:\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "        fig.suptitle(f\"sub-{subj}   |   ses-{sess}\", fontsize=12, weight=\"bold\")\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.94])\n",
    "\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "print(f\"All pages written to {pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataset_cache/montage_pngs/sub-BRA012_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-BRA012_ses-12.png\n",
      "Saved dataset_cache/montage_pngs/sub-BRA013_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-BRA013_ses-12.png\n",
      "Saved dataset_cache/montage_pngs/sub-BRA033_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-BRA033_ses-12.png\n",
      "Saved dataset_cache/montage_pngs/sub-BRA041_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-BRA041_ses-12.png\n",
      "Saved dataset_cache/montage_pngs/sub-BRA043_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-BRA043_ses-12.png\n",
      "Saved dataset_cache/montage_pngs/sub-BRA047_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-BRA047_ses-12.png\n",
      "Saved dataset_cache/montage_pngs/sub-BRA051_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-BRA051_ses-12.png\n",
      "Saved dataset_cache/montage_pngs/sub-BRA052_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-BRA052_ses-12.png\n",
      "Saved dataset_cache/montage_pngs/sub-BRA061_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-BRA061_ses-12.png\n",
      "Saved dataset_cache/montage_pngs/sub-BRA063_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-BRA063_ses-12.png\n",
      "Saved dataset_cache/montage_pngs/sub-BRA074_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-BRA074_ses-12.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM005_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM005_ses-12.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM010_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM010_ses-12.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM013_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM013_ses-12.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM020_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM020_ses-12.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM022_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM022_ses-12.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM028_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM028_ses-12.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM034_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM034_ses-12.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM104_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM104_ses-12.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM117_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM117_ses-12.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM120_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM120_ses-12.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM132_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM132_ses-12.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM137_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM137_ses-12.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM140_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM140_ses-12.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM146_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM146_ses-12.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM163_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM163_ses-12.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM164_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM164_ses-12.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM168_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM168_ses-12.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM179_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM179_ses-12.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM201_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM201_ses-12.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM213_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM213_ses-12.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM229_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM229_ses-12.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM234_ses-00.png\n",
      "Saved dataset_cache/montage_pngs/sub-VUM234_ses-12.png\n",
      "\n",
      "All compare PNGs in /nfs/ForHenry/brain_ventricle/dataset_cache/montage_pngs\n"
     ]
    }
   ],
   "source": [
    "def five_slice_lists_center(shape3, step1: int = 5, step2: int = 10, ax_shift: int = 0):\n",
    "    idx_all = []\n",
    "    for dim_i, dim in enumerate(shape3):\n",
    "        c = dim // 2\n",
    "        offsets = [-step2, -step1, 0, step1, step2]\n",
    "        if dim_i == 2:\n",
    "            c += ax_shift\n",
    "        idx = [min(max(c + off, 0), dim - 1) for off in offsets]\n",
    "        seen = set()\n",
    "        idx = [x for x in idx if not (x in seen or seen.add(x))]\n",
    "        k = 1\n",
    "        while len(idx) < 5:\n",
    "            for off in [step2 + k, -(step2 + k)]:\n",
    "                cand = min(max(c + off, 0), dim - 1)\n",
    "                if cand not in idx:\n",
    "                    idx.append(cand)\n",
    "                    if len(idx) == 5:\n",
    "                        break\n",
    "            k += 1\n",
    "        idx_all.append(np.array(idx[:5]))\n",
    "    return idx_all\n",
    "\n",
    "\n",
    "def load_to_ras(nifti_path: Path):\n",
    "    img = nib.load(str(nifti_path), mmap=True)\n",
    "    data_native = img.get_fdata(dtype=np.float32)\n",
    "\n",
    "    src_ornt = io_orientation(img.affine)\n",
    "    tgt_ornt = axcodes2ornt((\"R\", \"A\", \"S\"))\n",
    "    transform = ornt_transform(src_ornt, tgt_ornt)\n",
    "    data_ras = apply_orientation(data_native, transform)\n",
    "    return data_ras\n",
    "\n",
    "\n",
    "def extract_slice_ras(img_ras, axis, idx):\n",
    "    if axis == 0:\n",
    "        return np.rot90(img_ras[idx, :, :])\n",
    "    if axis == 1:\n",
    "        return np.rot90(img_ras[:, idx, :])\n",
    "    return np.rot90(img_ras[:, :, idx])\n",
    "\n",
    "\n",
    "root_out = Path(f\"{result_dir}/montage_pngs\")\n",
    "root_out.mkdir(exist_ok=True)\n",
    "\n",
    "for f in df[\"filepath\"]:\n",
    "    p = Path(f)\n",
    "    subj, sess = p.parts[6], p.parts[7]\n",
    "    ori_path = Path(*p.parts[:8]) / \"nii\" / (p.parts[9].split(\"_n4_reg\")[0] + \".nii.gz\")\n",
    "\n",
    "    img_ori_ras = load_to_ras(ori_path)\n",
    "    img_ham_ras = load_to_ras(p)\n",
    "\n",
    "    vmin_o, vmax_o = np.percentile(img_ori_ras, (1, 99))\n",
    "    vmin_h, vmax_h = np.percentile(img_ham_ras, (1, 99))\n",
    "    sag_o, cor_o, ax_o = five_slice_lists_center(img_ori_ras.shape, ax_shift=25)\n",
    "    sag_h, cor_h, ax_h = five_slice_lists_center(img_ham_ras.shape)\n",
    "\n",
    "    fig, axes = plt.subplots(6, 5, figsize=(11, 13), dpi=100)\n",
    "    for ax in axes.ravel():\n",
    "        ax.axis(\"off\")\n",
    "    for k, idx in enumerate(np.concatenate([sag_o, cor_o, ax_o])):\n",
    "        axes[k // 5, k % 5].imshow(\n",
    "            extract_slice_ras(\n",
    "                img_ori_ras, axis=0 if k < 5 else 1 if k < 10 else 2, idx=idx\n",
    "            ),\n",
    "            cmap=\"gray\",\n",
    "            vmin=vmin_o,\n",
    "            vmax=vmax_o,\n",
    "        )\n",
    "    for k, idx in enumerate(np.concatenate([sag_h, cor_h, ax_h]), start=15):\n",
    "        r, c = divmod(k, 5)\n",
    "        axes[r, c].imshow(\n",
    "            extract_slice_ras(\n",
    "                img_ham_ras, axis=0 if k < 20 else 1 if k < 25 else 2, idx=idx\n",
    "            ),\n",
    "            cmap=\"gray\",\n",
    "            vmin=vmin_h,\n",
    "            vmax=vmax_h,\n",
    "        )\n",
    "    axes[0, 0].set_title(\"Original T1 Image\", fontsize=12, loc=\"center\")\n",
    "    axes[3, 0].set_title(\"Harmonized T1 Image\", fontsize=12, loc=\"center\")\n",
    "    fig.suptitle(f\"sub-{subj} | ses-{sess}\", fontsize=14, weight=\"bold\")\n",
    "    plt.tight_layout(rect=[0.03, 0, 0.97, 0.95])\n",
    "    fig.savefig(root_out / f\"sub-{subj}_ses-{sess}.png\", dpi=100, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(\"Saved\", root_out / f\"sub-{subj}_ses-{sess}.png\")\n",
    "\n",
    "print(\"\\nAll compare PNGs in\", root_out.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
