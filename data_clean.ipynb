{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, os, time, re, csv, sys, io\n",
    "\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "from visualization import *\n",
    "\n",
    "root_dir = \"/valiant02/masi/zuol1/data/cibs_brain2/\"\n",
    "dest_root_dir = \"/nfs2/harmonization/BIDS/CIBS_BRAIN2_Harmonized\"\n",
    "demo_metadata_csv = \"labels/demo_metadata.csv\"\n",
    "result_dir = \"dataset_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump(path_list, outfile):\n",
    "    with open(outfile, \"w\") as f:\n",
    "        for p in sorted(path_list):\n",
    "            f.write(str(p) + \"\\n\")\n",
    "    print(f\"Cached {len(path_list)} paths to '{outfile}'\")\n",
    "\n",
    "\n",
    "def load_paths_from_file(file_path: str) -> List[str]:\n",
    "    \"\"\"Load paths from a file.\"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return [line.strip() for line in f if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached 3730 paths to 'dataset_cache/nifti_all.txt'\n"
     ]
    }
   ],
   "source": [
    "lines = []\n",
    "for dirpath, dirnames, filenames in os.walk(Path(root_dir)):\n",
    "    if dirnames:\n",
    "        continue\n",
    "    file_type = Path(dirpath).name\n",
    "    session = Path(dirpath).parent.name\n",
    "    if session not in {\"00\", \"12\"}:\n",
    "        continue\n",
    "    for fname in filenames:\n",
    "        if fname.endswith(\".nii\") or fname.endswith(\".nii.gz\"):\n",
    "            lines.append(str(Path(dirpath) / fname))\n",
    "dump(lines, f\"{result_dir}/nifti_all.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 722 paths from nifti_nii.txt\n"
     ]
    }
   ],
   "source": [
    "lines = load_paths_from_file(f\"{result_dir}/nifti_nii.txt\")\n",
    "print(f\"Loaded {len(lines)} paths from nifti_nii.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "depths = [len(Path(p).parts) for p in lines]\n",
    "\n",
    "# Check whether the depths of all addresses are same\n",
    "print(len(set(depths)) == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached 76 paths to 'dataset_cache/subject_only_00.txt'\n",
      "Cached 37 paths to 'dataset_cache/subject_00_and_12.txt'\n",
      "Cached 22 paths to 'dataset_cache/subject_only_12.txt'\n",
      "Only baseline data (00 only): 76\n",
      "Valid data (00 + 12): 37\n",
      "Only 12-month data: 22\n"
     ]
    }
   ],
   "source": [
    "subject_sessions = defaultdict(set)\n",
    "\n",
    "for fp in lines:\n",
    "    p = Path(fp)\n",
    "    if len(p.parents) < 3:\n",
    "        continue\n",
    "    session = p.parents[1].name\n",
    "    file_type = p.parents[0].name\n",
    "    if session not in [\"00\", \"12\"] or file_type != \"nii\":\n",
    "        continue\n",
    "    subject_dir = p.parents[2]\n",
    "    subject_sessions[subject_dir].add(session)\n",
    "\n",
    "only_00 = [p for p, s in subject_sessions.items() if s == {\"00\"}]\n",
    "both_00_12 = [p for p, s in subject_sessions.items() if s == {\"00\", \"12\"}]\n",
    "only_12 = [p for p, s in subject_sessions.items() if s == {\"12\"}]\n",
    "\n",
    "dump(only_00, f\"{result_dir}/subject_only_00.txt\")\n",
    "dump(both_00_12, f\"{result_dir}/subject_00_and_12.txt\")\n",
    "dump(only_12, f\"{result_dir}/subject_only_12.txt\")\n",
    "\n",
    "print(f\"Only baseline data (00 only): {len(only_00)}\")\n",
    "print(f\"Valid data (00 + 12): {len(both_00_12)}\")\n",
    "print(f\"Only 12-month data: {len(only_12)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached 168 paths to 'dataset_cache/harmonized_all.txt'\n",
      "\n",
      " Subjects with missing data:\n",
      "  - BRA105: no BRAIN_T1-3D harmonized file in 00/proc\n",
      "  - BRA107: no BRAIN_T1-3D harmonized file in 12/proc\n",
      "  - BRA132: no BRAIN_T1-3D harmonized file in 12/proc\n",
      "  - VUM241: no BRAIN_T1-3D harmonized file in 12/proc\n"
     ]
    }
   ],
   "source": [
    "file_pat = re.compile(r\"BRAIN_T1-3D.*_n4_reg_harmonized_harmonized_fusion\\.nii\\.gz$\")\n",
    "\n",
    "good_paths: list[Path] = []\n",
    "bad_subjects: list[str] = []\n",
    "subject_roots = only_00 + both_00_12 + only_12\n",
    "\n",
    "for subj_root in subject_roots:\n",
    "    subj_root = Path(subj_root)\n",
    "    subj_id = subj_root.name\n",
    "\n",
    "    for ses in (\"00\", \"12\"):\n",
    "        ses_dir = subj_root / ses\n",
    "        if not ses_dir.is_dir():\n",
    "            continue\n",
    "        proc_dir = ses_dir / \"proc\"\n",
    "        if not proc_dir.is_dir():\n",
    "            bad_subjects.append(f\"{subj_id}: missing directory {ses}/proc\")\n",
    "            break\n",
    "        matches = [\n",
    "            p for p in proc_dir.iterdir() if p.is_file() and file_pat.search(p.name)\n",
    "        ]\n",
    "        if not matches:\n",
    "            bad_subjects.append(\n",
    "                f\"{subj_id}: no BRAIN_T1-3D harmonized file in {ses}/proc\"\n",
    "            )\n",
    "            break\n",
    "        good_paths.extend(matches)\n",
    "\n",
    "dump(good_paths, f\"{result_dir}/harmonized_all.txt\")\n",
    "\n",
    "if bad_subjects:\n",
    "    print(\"\\n Subjects with missing data:\")\n",
    "    for msg in bad_subjects:\n",
    "        print(\"  -\", msg)\n",
    "else:\n",
    "    print(\"\\n All subjects passed the file-check.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 168 paths from harmonized_all.txt\n"
     ]
    }
   ],
   "source": [
    "good_paths = load_paths_from_file(f\"{result_dir}/harmonized_all.txt\")\n",
    "print(f\"Loaded {len(good_paths)} paths from harmonized_all.txt\")\n",
    "\n",
    "\n",
    "def extract_modality(fname: str) -> str:\n",
    "    stem = fname\n",
    "    for ext in (\".nii.gz\", \".nii\"):\n",
    "        if stem.endswith(ext):\n",
    "            stem = stem[: -len(ext)]\n",
    "            break\n",
    "    try:\n",
    "        seq_part = stem.split(\"_BRAIN_\")[1]\n",
    "    except IndexError:\n",
    "        return \"\"\n",
    "    tokens = seq_part.split(\"-\")\n",
    "    return \"-\".join(tokens[:2]) if len(tokens) >= 2 else \"\"\n",
    "\n",
    "\n",
    "def parse_path(file_path: str) -> dict | None:\n",
    "    p = Path(file_path)\n",
    "    parts = p.parts\n",
    "    if len(parts) != 10:\n",
    "        raise RuntimeError(f\"File address not valid.\\nAddress: {file_path}\")\n",
    "    subject = parts[6]\n",
    "    session = parts[7]\n",
    "    scan_type = extract_modality(parts[9])\n",
    "    if scan_type not in {\"T1-3D\"}:\n",
    "        return None\n",
    "    return {\n",
    "        \"filepath\": p.resolve(),\n",
    "        \"scan_type\": scan_type,\n",
    "        \"subject_id\": f\"sub-{subject}\",\n",
    "        \"session_id\": f\"ses-{session}\",\n",
    "    }\n",
    "\n",
    "\n",
    "rows = [row for fp in good_paths if (row := parse_path(fp))]\n",
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"run\"] = (\n",
    "    df.groupby([\"scan_type\", \"subject_id\", \"session_id\"])\n",
    "    .cumcount()\n",
    "    .add(1)\n",
    "    .astype(\"string\")\n",
    ")\n",
    "\n",
    "mask = (\n",
    "    df.groupby([\"scan_type\", \"subject_id\", \"session_id\"])[\"run\"].transform(\"size\").eq(1)\n",
    ")\n",
    "df.loc[mask, \"run\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_links(row):\n",
    "    prefix = f\"{row.subject_id}_{row.session_id}\"\n",
    "    if row.run:\n",
    "        prefix += f\"_run-{row.run}\"\n",
    "    if row.scan_type == \"T1-3D\":\n",
    "        fname = f\"{prefix}_T1w\"\n",
    "        subdir = \"anat\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scan type: {row.scan_type}\")\n",
    "    base = f\"{dest_root_dir}/{row.subject_id}/{row.session_id}/{subdir}/{fname}\"\n",
    "    row[\"nii_link\"] = base + \".nii.gz\"\n",
    "    return row\n",
    "\n",
    "\n",
    "df = df.apply(make_links, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values([\"scan_type\", \"subject_id\", \"session_id\", \"run\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing here.\n"
     ]
    }
   ],
   "source": [
    "mask = (\n",
    "    (df[\"scan_type\"] == \"T1-3D\")\n",
    "    & df[\"run\"].notna()\n",
    "    & (df[\"run\"] != \"\")\n",
    "    # & (df[\"run\"] == \"\")\n",
    ")\n",
    "\n",
    "if mask.any():\n",
    "    print(df.loc[mask])\n",
    "else:\n",
    "    print(\"Nothing here.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"{result_dir}/data.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframe(path: str) -> pd.DataFrame:\n",
    "    if path.endswith(\".pkl\"):\n",
    "        return pd.read_pickle(path)\n",
    "    return pd.read_csv(path, sep=None, engine=\"python\")\n",
    "\n",
    "\n",
    "def _collect_link_pairs(df: pd.DataFrame) -> List[Tuple[str, str]]:\n",
    "    colmap = {\n",
    "        \"filepath\": \"nii_link\",\n",
    "    }\n",
    "\n",
    "    present_pairs = [\n",
    "        (src, dst)\n",
    "        for src, dst in colmap.items()\n",
    "        if src in df.columns and dst in df.columns\n",
    "    ]\n",
    "    if not present_pairs:\n",
    "        raise ValueError(\"No recognised source/target column pairs found.\")\n",
    "\n",
    "    commands = []\n",
    "    for _, row in df.iterrows():\n",
    "        for src, dst in present_pairs:\n",
    "            s, d = row[src], row[dst]\n",
    "            if pd.notna(s) and pd.notna(d) and str(s).strip() and str(d).strip():\n",
    "                commands.append(f'mkdir -p \"{Path(d).parent}\"')\n",
    "                commands.append(f'ln -s \"{s}\" \"{d}\"')\n",
    "    return commands\n",
    "\n",
    "\n",
    "def write_link_commands(\n",
    "    df: pd.DataFrame,\n",
    "    output_txt: str | Path,\n",
    "    *,\n",
    "    overwrite: bool = True,\n",
    ") -> None:\n",
    "    output_txt = Path(output_txt)\n",
    "    mode = \"w\" if overwrite else \"a\"\n",
    "\n",
    "    commands = _collect_link_pairs(df)\n",
    "\n",
    "    with output_txt.open(mode, encoding=\"utf-8\") as f:\n",
    "        for cmd in commands:\n",
    "            f.write(cmd + \"\\n\")\n",
    "\n",
    "    print(f\"{len(commands)} link commands written to {output_txt.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataframe(f\"{result_dir}/data.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336 link commands written to /nfs/ForHenry/brain_ventricle/dataset_cache/data_link_command.txt\n"
     ]
    }
   ],
   "source": [
    "write_link_commands(df, f\"{result_dir}/data_link_command.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vum_meta_df = pd.read_csv(\"labels/vum_metadata.csv\", dtype={\"SessionID\": str})\n",
    "\n",
    "only_00 = load_paths_from_file(f\"{result_dir}/subject_only_00.txt\")\n",
    "both_00_12 = load_paths_from_file(f\"{result_dir}/subject_00_and_12.txt\")\n",
    "only_12 = load_paths_from_file(f\"{result_dir}/subject_only_12.txt\")\n",
    "\n",
    "checks = [\n",
    "    (\"only_00\", only_00, [\"00\"]),\n",
    "    (\"both_00_12\", both_00_12, [\"00\", \"12\"]),\n",
    "    (\"only_12\", only_12, [\"12\"]),\n",
    "]\n",
    "\n",
    "for label, subject_list, sessions in checks:\n",
    "    for subject_addr in subject_list:\n",
    "        subject_id = Path(subject_addr).name\n",
    "        if not subject_id.startswith(\"VUM\"):\n",
    "            continue\n",
    "        for ses in sessions:\n",
    "            mask = (vum_meta_df[\"VUM_ID\"] == subject_id) & (\n",
    "                vum_meta_df[\"SessionID\"] == ses\n",
    "            )\n",
    "            if not mask.any():\n",
    "                print(f\"Missing metadata for {subject_id} session {ses}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
